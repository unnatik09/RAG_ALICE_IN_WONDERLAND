# ğŸ“š Alice in Wonderland - RAG Application

This project demonstrates a **Retrieval-Augmented Generation (RAG) application**.  
The system ingests *Alice's Adventures in Wonderland* by Lewis Carroll (public domain ebook), processes it into embeddings, stores them in a vector database, and enables **context-aware question answering**.

---

## ğŸš€ Features

- ğŸ“– Uses **Alice in Wonderland** as the knowledge base  
- ğŸ§© **Text chunking** for efficient retrieval  
- ğŸ” **Vector embeddings** to enable semantic search  
- ğŸ“‚ **Vector store (FAISS)** for storing and retrieving chunks  
- â“ Ask natural language questions about the book and get accurate, grounded answers  
- ğŸ”§ Fully customizable â€” swap datasets, embeddings, or models  

---

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**  
- **LangChain** â€“ orchestration framework  
- **FAISS** â€“ vector store for retrieval  
- **SentenceTransformers** â€“ embeddings (`all-MiniLM-L6-v2`)  
- **OpenAI API** â€“ for LLM responses  
- **Jupyter Notebook / Streamlit** â€“ interface options  

---

## ğŸ“‚ Project Structure
alice-rag/  
â”‚â”€â”€ data/  
â”‚   â””â”€â”€ alice_in_wonderland.txt   # The ebook (public domain)  
â”‚â”€â”€ notebooks/  
â”‚   â””â”€â”€ rag_pipeline.ipynb        # End-to-end RAG pipeline  
â”‚â”€â”€ app.py                        # Optional Streamlit/Gradio interface  
â”‚â”€â”€ requirements.txt              # Python dependencies  
â”‚â”€â”€ README.md                     # Project documentation  

---

## âš¡ How It Works

1. **Load Dataset**  
   Import *Alice in Wonderland* text.  

2. **Preprocess & Chunk**  
   Split the text into manageable chunks (e.g., 500 tokens).  

3. **Generate Embeddings**  
   Use `SentenceTransformers` to embed chunks.  

4. **Store in Vector DB**  
   Save embeddings in **FAISS** for fast similarity search.  

5. **Query**  
   User asks a question â†’ query is embedded â†’ FAISS retrieves relevant passages â†’ OpenAI API generates the final answer.  

6. **Answer Generation**  
   A local LLM (HuggingFace / Ollama) generates an answer grounded in retrieved passages.

---
